The HW6 Colab file is in this folder (HW5), which is on the Gaussian Process Regression to predict El Ni√±o sea surface temperature for each day from January 1, 1950, to December 1, 2030, using measured monthly averaged temperature data from 1950 to 2010. Understanding the Gaussian Process is really tricky for me, and I had to look up the george documentation and the tutorial to find my way through. Currently, as I did this homework, I started to understand more about the implementation of this Gaussian process than previously, but I still don't feel like I fully understand the mechanics behind it. Though, discussions with peers and asking Willow has helped me understand the gist quite better. 
In the visualization part of this homework, I initially only had a histogram plot, but then realized I needed more variety in plots to visualize the dataset better, especially to see its periodicity. Hence, I tried doing a heatmap and a way to visualize the temperature oscillations within a year, that would happen every year. Masooma helped me and taught me how to plot the heatmap and also a boxplot using the seaborn package, which seems very useful. To work on the flattening data function, I also experimented on how to represent the time data a few ways--I started by writing them as strings indicating year and month, but at the end decided to write them in terms of integers taht would mark the middle day of each month because the former would be complicated to run on the GP regression function. 
In the GPR function of part 2, I ended up creating the interpolated time array with a similar approach as I did with the flattened time data; by marking them as integers indicating each day throughout the time period. Running the GP function itself initially did not work (most of the times I troubleshoot this, I was getting either a huge spike in temperature right after the measured times, or that my predicted data was not oscillating enough), so Tali suggested me to normalize my time arrays. It also turned out that the initial guess values would determine the shape of the outcome, so I had to look for optimal values for this value (which would be parameters of the kernels) by a lot of trial and error. I also tried switching kernels to superpositions and combinations between exp sine 2 kernel, constant kernel, and one of the matern kernels, but it did not improve that much from what I had inputted. 
Setting the parameter optimization function also gave me quite a hard time--I tried following what was done in the george tutorial, but I was very confused on what was happening in it. Hence, I went to office hours to ask about this, and Willow helped clear this up to me. Though, when I set and ran the optimizer function in my homework, it was still giving me errors, as it turns out some pararmeters in my function weren't well-defined, and that I still had to run the gp.compute and gp.predict in order to optimize. Sarah helped look at my code and fix the errors, and Tali also largely helped me with this part as I ended up having more errors when I messed around more with the rest of the plots, the kernels, and the initial guesses. At the end, in visualizing the data, I checked what plots I had with what Tali had, as we had the same kernel, to make sure I wasn't making a huge mistake somewhere else. I also checked my plots with Masooma. In doing this homework, I also talked to Zack, Miles, Shar, Sid, and Paula especially about checking my values and plots.
